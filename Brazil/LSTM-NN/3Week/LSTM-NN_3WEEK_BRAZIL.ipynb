{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM-NN_3WEEK_BRAZIL.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"KBSc2Ol3Xy_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"c4ff6c98-0dca-4bc4-fc81-302ca7fdffa2","executionInfo":{"status":"ok","timestamp":1550825021742,"user_tz":360,"elapsed":278,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"MjlOwbDAXwz0","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O8MmzreEXw0H","colab_type":"code","colab":{}},"cell_type":"code","source":["files = os.listdir(\"drive/My Drive/ziknet-trends-rolling/data/Brazil/processed_data\")\n","weeks_ahead = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qh4Hg7E-Xw0Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":752},"outputId":"a3f62b3a-5748-43ad-9699-3e9a7f0315ef","executionInfo":{"status":"ok","timestamp":1550825022084,"user_tz":360,"elapsed":581,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}}},"cell_type":"code","source":["datasets = {}\n","for file in files:\n","    datasets[file] = pd.read_csv(\"drive/My Drive/ziknet-trends-rolling/data/Brazil/processed_data/{}\".format(file))\n","    datasets[file][\"Searches\"] /= 100\n","    print(file)\n","    print(datasets[file].tail())\n","    print(\"Number of observations: \", len(datasets[file]))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Bahia_2016-2017.csv\n","           Date  Searches  Cases\n","99   27/11/2017      0.26   19.0\n","100  04/12/2017      0.30   30.0\n","101  11/12/2017      0.09    1.0\n","102  18/12/2017      0.30    3.0\n","103  25/12/2017      0.28   56.0\n","Number of observations:  104\n","MatoGrosso_2016-2017.csv\n","           Date  Searches  Cases\n","99   27/11/2017      0.66    0.0\n","100  04/12/2017      0.00  -25.0\n","101  11/12/2017      0.55   19.0\n","102  18/12/2017      0.00    1.0\n","103  25/12/2017      0.68   -1.0\n","Number of observations:  104\n","MinasGerais_2016-2017.csv\n","           Date  Searches  Cases\n","99   27/11/2017      0.21    3.0\n","100  04/12/2017      0.17    3.0\n","101  11/12/2017      0.09    3.0\n","102  18/12/2017      0.16    9.0\n","103  25/12/2017      0.21    8.0\n","Number of observations:  104\n","RioDeJaneiro_2016-2017.csv\n","           Date  Searches  Cases\n","99   27/11/2017      0.22    0.0\n","100  04/12/2017      0.18    0.0\n","101  11/12/2017      0.20    0.0\n","102  18/12/2017      0.19    0.0\n","103  25/12/2017      0.23    0.0\n","Number of observations:  104\n","SaoPaulo_2016-2017.csv\n","           Date  Searches  Cases\n","99   27/11/2017      0.13    9.0\n","100  04/12/2017      0.18    4.0\n","101  11/12/2017      0.17   -4.0\n","102  18/12/2017      0.14    1.0\n","103  25/12/2017      0.29   15.0\n","Number of observations:  104\n"],"name":"stdout"}]},{"metadata":{"id":"FidFrrgHXw0h","colab_type":"code","colab":{}},"cell_type":"code","source":["def series_to_supervised(df, outputColumn, n_in=1, n_out=1, dropnan=True):\n","    n_vars = df.shape[1]\n","    cols, names = list(), list()\n","    # input sequence (t-n, ... t-1)\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [(\"{}(t-{})\".format(col, i)) for col in df.columns]\n","\n","    # Append next observation[outputColumn] at n_out obs\n","    cols.append(df[outputColumn].shift(-n_out+1))\n","    names+=[outputColumn + \"(t+{})\".format(n_out-1)]\n","\n","    # put it all together\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    # drop rows with NaN values\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Np8NAYnMXw0q","colab_type":"code","colab":{}},"cell_type":"code","source":["def getXY(dataset, state, weeksAhead):\n","    n_features = dataset.shape[1]\n","    \n","    n_weeks = 4\n","    reframed = series_to_supervised(dataset, \"Cases\",  n_weeks, weeksAhead)\n","    values = reframed.values\n","    \n","    totalFeatures = values.shape[1]\n","\n","    x,y = values[:, :totalFeatures-1], values[:, totalFeatures-1] #Y is the last column, X is all the previous columns \n","\n","    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n","    return x, y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PmHtWKQrXw0x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"bf8c4fc0-ebd1-48a1-bbdc-5153cd899c5a","executionInfo":{"status":"ok","timestamp":1550825023112,"user_tz":360,"elapsed":1582,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}}},"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Flatten, Dense, LSTM, BatchNormalization\n","from keras.optimizers import Adam\n","from keras.layers.merge import concatenate\n","from keras.constraints import non_neg\n","def LSTM_NN_Model():\n","    input_layer = Input(shape=(4,2))\n","    \n","    \n","    b1_out = LSTM(16, return_sequences=False)(input_layer)\n","\n","    b2_out = Dense(128, activation=\"relu\", kernel_regularizer=\"l2\")(input_layer)\n","    b2_out = Flatten()(b2_out)\n","\n","    concatenated = concatenate([b1_out, b2_out])\n","    \n","    out = Dense(8, activation=\"relu\", kernel_regularizer=\"l2\")(concatenated)\n","    out = Dense(8, activation=\"relu\", kernel_regularizer=\"l2\")(out)\n","#     out = Dense(1, activation=\"linear\", kernel_constraint=non_neg(), name='output_layer')(out)\n","    out = Dense(1, activation=\"linear\", name='output_layer')(out)\n","\n","    model = Model([input_layer], out)\n","    model.compile(loss=[\"mse\"], optimizer=Adam(0.0001), metrics=[\"mae\"])\n","\n","    return model"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"u-Jux_TxXw06","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":239},"outputId":"59a158c6-55b3-4afd-f86c-5a3238582a58","executionInfo":{"status":"ok","timestamp":1550826093670,"user_tz":360,"elapsed":1072135,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}}},"cell_type":"code","source":["for file in datasets:\n","    dataset = datasets[file]\n","    x, y = getXY(dataset[[\"Searches\", \"Cases\"]], \"\", weeks_ahead)\n","    model = LSTM_NN_Model()\n","    \n","    xDim1 = x[0].shape[0]\n","    xDim2 = x[0].shape[1]\n","    \n","    splitIndex = len(x) - 52\n","\n","    train_X = x[:splitIndex]\n","    train_y = y[:splitIndex]\n","\n","    test_y = y[splitIndex:]\n","    outDataset = pd.DataFrame()\n","    outDataset[\"Observerd\"] = test_y\n","\n","    model.fit(\n","        train_X,\n","        train_y,\n","        epochs=200,\n","        batch_size=4,\n","        verbose=0,\n","        shuffle=False)\n","\n","    predicted_y_history = []\n","\n","    while(splitIndex < len(y)):\n","        predicted_y = \\\n","            model.predict(x[splitIndex].reshape(1, xDim1, xDim2))[0]\n","\n","        model.fit(\n","            x[:splitIndex+1],\n","            y[:splitIndex+1],\n","            epochs=40,\n","            batch_size=4,\n","            verbose=0,\n","            shuffle=False)\n","\n","        predicted_y_history.append(predicted_y[0])\n","        splitIndex += 1\n","\n","    outDataset[\"PREDICTED\"] = predicted_y_history\n","    print(\"{} MSE: {}\".format(file, mean_squared_error(test_y, predicted_y_history)))\n","    outDataset.to_csv(file)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Bahia_2016-2017.csv MSE: 3075.6180316059585\n","MatoGrosso_2016-2017.csv MSE: 2930.086443905383\n","MinasGerais_2016-2017.csv MSE: 473.2188486140362\n","RioDeJaneiro_2016-2017.csv MSE: 6899.086491715281\n","SaoPaulo_2016-2017.csv MSE: 138.64155935476032\n"],"name":"stdout"}]}]}