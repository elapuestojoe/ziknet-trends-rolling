{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM-NN_1WEEK_COLOMBIA.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"oq5EIhnsVqdV","colab_type":"code","outputId":"c103804c-66a5-4427-f403-470a3160124b","executionInfo":{"status":"ok","timestamp":1550855936945,"user_tz":360,"elapsed":391,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"x09B5VYdVZrB","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c9q24qZrVZrS","colab_type":"code","colab":{}},"cell_type":"code","source":["files = os.listdir(\"drive/My Drive/ziknet-trends-rolling/data/Colombia/processed_data\")\n","weeks_ahead = 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6hAk2XFGVZrb","colab_type":"code","outputId":"51f9dc56-8c7e-458c-e5b9-863bbc86b842","executionInfo":{"status":"ok","timestamp":1550855939517,"user_tz":360,"elapsed":2921,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}},"colab":{"base_uri":"https://localhost:8080/","height":752}},"cell_type":"code","source":["datasets = {}\n","minNDictionary = {}\n","for file in files:\n","    datasets[file] = pd.read_csv(\"drive/My Drive/ziknet-trends-rolling/data/Colombia/processed_data/{}\".format(file))\n","    datasets[file][\"Searches\"] /= 100\n","    print(file)\n","    print(datasets[file].tail())\n","    print(\"Number of observations: \", len(datasets[file]))\n","\n","    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["huila_2016-2017.csv\n","           Date  Searches  Cases\n","99   26/11/2017      1.00     -1\n","100  03/12/2017      1.00      1\n","101  10/12/2017      0.00      1\n","102  17/12/2017      0.71      0\n","103  24/12/2017      0.00      0\n","Number of observations:  104\n","santander_2016-2017.csv\n","           Date  Searches  Cases\n","99   26/11/2017      0.30     -3\n","100  03/12/2017      0.00      7\n","101  10/12/2017      0.02      4\n","102  17/12/2017      0.00     -1\n","103  24/12/2017      0.33     -5\n","Number of observations:  104\n","santander_norte_2016-2017.csv\n","           Date  Searches  Cases\n","99   26/11/2017      0.28      0\n","100  03/12/2017      0.00      1\n","101  10/12/2017      0.04      4\n","102  17/12/2017      0.00      4\n","103  24/12/2017      0.65      3\n","Number of observations:  104\n","tolima_2016-2017.csv\n","           Date  Searches  Cases\n","99   26/11/2017      0.36     -1\n","100  03/12/2017      0.00      2\n","101  10/12/2017      0.00      2\n","102  17/12/2017      0.00     -1\n","103  24/12/2017      0.00      2\n","Number of observations:  104\n","valle_cauca_2016-2017.csv\n","           Date  Searches  Cases\n","99   26/11/2017      0.21    -12\n","100  03/12/2017      0.14     24\n","101  10/12/2017      0.01      8\n","102  17/12/2017      0.10    -49\n","103  24/12/2017      0.00      3\n","Number of observations:  104\n"],"name":"stdout"}]},{"metadata":{"id":"AN4uD7jOVZrp","colab_type":"code","colab":{}},"cell_type":"code","source":["def series_to_supervised(df, outputColumn, n_in=1, n_out=1, dropnan=True):\n","    n_vars = df.shape[1]\n","    cols, names = list(), list()\n","    # input sequence (t-n, ... t-1)\n","    for i in range(n_in, 0, -1):\n","        cols.append(df.shift(i))\n","        names += [(\"{}(t-{})\".format(col, i)) for col in df.columns]\n","\n","    # Append next observation[outputColumn] at n_out obs\n","    cols.append(df[outputColumn].shift(-n_out+1))\n","    names+=[outputColumn + \"(t+{})\".format(n_out-1)]\n","\n","    # put it all together\n","    agg = pd.concat(cols, axis=1)\n","    agg.columns = names\n","    # drop rows with NaN values\n","    if dropnan:\n","        agg.dropna(inplace=True)\n","    return agg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dIeFgsWcVZrw","colab_type":"code","colab":{}},"cell_type":"code","source":["def getXY(dataset, state, weeksAhead):\n","    n_features = dataset.shape[1]\n","    \n","    n_weeks = 4\n","    reframed = series_to_supervised(dataset, \"Cases\",  n_weeks, weeksAhead)\n","    values = reframed.values\n","    \n","    totalFeatures = values.shape[1]\n","\n","    x,y = values[:, :totalFeatures-1], values[:, totalFeatures-1] #Y is the last column, X is all the previous columns \n","\n","    x = x.reshape((x.shape[0], n_weeks, n_features)) # Reshape as 3-D\n","    return x, y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EeKl6snqVZr2","colab_type":"code","outputId":"b071d658-d09b-4646-ee93-05a7d4d9403d","executionInfo":{"status":"ok","timestamp":1550855940827,"user_tz":360,"elapsed":4208,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Flatten, Dense, LSTM, BatchNormalization\n","from keras.optimizers import Adam\n","from keras.layers.merge import concatenate\n","from keras.constraints import non_neg\n","def LSTM_NN_Model():\n","    input_layer = Input(shape=(4,2))\n","    \n","    \n","    b1_out = LSTM(16, return_sequences=False)(input_layer)\n","\n","    b2_out = Dense(128, activation=\"relu\", kernel_regularizer=\"l2\")(input_layer)\n","    b2_out = Flatten()(b2_out)\n","\n","    concatenated = concatenate([b1_out, b2_out])\n","    \n","    out = Dense(8, activation=\"relu\", kernel_regularizer=\"l2\")(concatenated)\n","    out = Dense(8, activation=\"relu\", kernel_regularizer=\"l2\")(out)\n","#     out = Dense(1, activation=\"linear\", kernel_constraint=non_neg(), name='output_layer')(out)\n","    out = Dense(1, activation=\"linear\", name='output_layer')(out)\n","\n","    model = Model([input_layer], out)\n","    model.compile(loss=[\"mse\"], optimizer=Adam(0.0001), metrics=[\"mae\"])\n","\n","    return model"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"TFVTuhXTVZsA","colab_type":"code","outputId":"a4294615-8628-4b0a-b770-4d4ccfc8b9f5","executionInfo":{"status":"ok","timestamp":1550858477539,"user_tz":360,"elapsed":2540904,"user":{"displayName":"Kevin Islas Abud","photoUrl":"","userId":"17803260883912783595"}},"colab":{"base_uri":"https://localhost:8080/","height":219}},"cell_type":"code","source":["for file in datasets:\n","    dataset = datasets[file]\n","    x, y = getXY(dataset[[\"Searches\", \"Cases\"]], \"\", weeks_ahead)\n","    model = LSTM_NN_Model()\n","    \n","    xDim1 = x[0].shape[0]\n","    xDim2 = x[0].shape[1]\n","    \n","    splitIndex = len(x) - 52\n","\n","    train_X = x[:splitIndex]\n","    train_y = y[:splitIndex]\n","\n","    test_y = y[splitIndex:]\n","    outDataset = pd.DataFrame()\n","    outDataset[\"Observerd\"] = test_y\n","\n","    model.fit(\n","        train_X,\n","        train_y,\n","        epochs=200,\n","        batch_size=4,\n","        verbose=0,\n","        shuffle=False)\n","\n","    predicted_y_history = []\n","\n","    while(splitIndex < len(y)):\n","        predicted_y = \\\n","            model.predict(x[splitIndex].reshape(1, xDim1, xDim2))[0]\n","        \n","        model.fit(\n","            x[:splitIndex+1],\n","            y[:splitIndex+1],\n","            epochs=40,\n","            batch_size=4,\n","            verbose=0,\n","            shuffle=False)\n","\n","        predicted_y_history.append(predicted_y[0])\n","        splitIndex += 1\n","    \n","#     predicted_y_history = np.exp(predicted_y_history)\n","#     predicted_y_history -= minNDictionary[file]\n","    \n","#     test_y = np.exp(test_y)\n","#     test_y -= minNDictionary[file]\n","    \n","    outDataset[\"PREDICTED\"] = predicted_y_history\n","    print(\"{} MSE: {}\".format(file, mean_squared_error(test_y, predicted_y_history)))\n","    outDataset.to_csv(file)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","huila_2016-2017.csv MSE: 16.99801327842565\n","santander_2016-2017.csv MSE: 43.68782661982746\n","santander_norte_2016-2017.csv MSE: 32.4248016301576\n","tolima_2016-2017.csv MSE: 46.652391162969636\n","valle_cauca_2016-2017.csv MSE: 956.9649447128177\n"],"name":"stdout"}]},{"metadata":{"id":"DsjoJkSAVZsJ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}